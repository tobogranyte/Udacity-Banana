{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Collector Agent Training Report\n",
    "\n",
    "The implementation for this project is almost line-for-line the same implemenation as used for the lunar lander coding project from earlier in the course. The only significant change is switching out the OpenAI environment for the Unity Banana environment provided for this project. \n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### Model - Best Performing\n",
    "\n",
    "In this case, the model for the network is a three layer fully-connected network. The first layer with 64 nodes takes 37 inputs corresponding to the state space. That layer is sent through a RELU non-linear function to the second layer, also with 64 nodes. That, in turn, is passed through another RELU layer to the third layer with four nodes corresponding to each action in the action space. The output of that layer is used directly as the action output without any further nonlinearization layers.\n",
    "\n",
    "### Model - Alternate (Deep)\n",
    "\n",
    "I set up an alternate version of the model, `model_deeper.py`. This model has five fully connected layers with 64, 128, 64, 32 and 4 nodes respectively. All have a RELU function on the output except for the final output layer.\n",
    "\n",
    "Surprisingly, this model generally performed more poorly on the task than the initial, shallower model. Results from this model have been included in the report along with the better-performing shallow model.\n",
    "\n",
    "### Process\n",
    "\n",
    "The learning algorithm is a basic DQN learning algorithm. There are two networks, the Local Network and the Target Network. Both start off initialized identically.\n",
    "\n",
    "After each action chosen by the model (Epsilon Start: 1.0, Epsilon Decay: 0.995, Epsilon End: 0.01) and environment update, the state, action, reward, next state and whether the episode is compelte are stored in the replay buffer. Once the replay buffer has enough tuples stored to generate a sample for batch learning (default batch size: 64), then every 4 steps, the agent would select a random batch-sized sample from the replay buffer. The local network's predicted Q value for the states selected from the replay buffer is compared to the target network's discounted maximum predicted Q value for the next state's actions plus actual rewards for the current states (Bellman equation) in the buffer and the MSE is calculated.  The MSE is the loss function to optimize the local network using the Adam optimizer with a learning rate of 5e-4.\n",
    "\n",
    "After optimizing the local network, the target network would then be updated with Tau = 1e-3.\n",
    "\n",
    "## Results\n",
    "\n",
    "In addition to the raw scores per episode, I also created a plot that contained the average score over the last 100 episodes (black line) to get a clearer picture of the learning trajectory. That plot also includes the rolling average of total yellow bananas collected (yellow) and blue bananas collected (blue) in order to get an idea of the raw numbers (averaged over 100 episodes) of each type of banana collected. This was to get an idea of how much further improvement was possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
