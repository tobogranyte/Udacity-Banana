{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Collector Agent Training Report\n",
    "\n",
    "The implementation for this project is almost line-for-line the same implemenation as used for the lunar lander coding project from earlier in the course. The only significant change is switching out the OpenAI environment for the Unity Banana environment provided for this project. \n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### Model - Best Performing\n",
    "\n",
    "In this case, the model for the network is a three layer fully-connected network. The first layer with 64 nodes takes 37 inputs corresponding to the state space. That layer is sent through a RELU non-linear function to the second layer, also with 64 nodes. That, in turn, is passed through another RELU layer to the third layer with four nodes corresponding to each action in the action space. The output of that layer is used directly as the action output without any further nonlinearization layers.\n",
    "\n",
    "### Model - Alternate (Deep)\n",
    "\n",
    "I set up an alternate version of the model, `model_deeper.py`. This model has five fully connected layers with 64, 128, 64, 32 and 4 nodes respectively. All have a RELU function on the output except for the final output layer.\n",
    "\n",
    "Surprisingly, this model generally performed more poorly on the task than the initial, shallower model. Results from this model have been included in the report along with the better-performing shallow model.\n",
    "\n",
    "### Process\n",
    "\n",
    "The learning algorithm is a basic DQN learning algorithm. There are two networks, the Local Network and the Target Network. Both start off initialized identically.\n",
    "\n",
    "After each action chosen by the model (Epsilon Start: 1.0, Epsilon Decay: 0.995, Epsilon End: 0.01) and environment update, the state, action, reward, next state and whether the episode is compelte are stored in the replay buffer. Once the replay buffer has enough tuples stored to generate a sample for batch learning (default batch size: 64), then every 4 steps, the agent would select a random batch-sized sample from the replay buffer. The local network's predicted Q value for the states selected from the replay buffer is compared to the target network's discounted maximum predicted Q value for the next state's actions plus actual rewards for the current states (Bellman equation) in the buffer and the MSE is calculated.  The MSE is the loss function to optimize the local network using the Adam optimizer with a learning rate of 5e-4.\n",
    "\n",
    "After optimizing the local network, the target network would then be updated with Tau = 1e-3.\n",
    "\n",
    "## Results\n",
    "\n",
    "In addition to the raw scores per episode, I also created a plot that contained the average score over the last 100 episodes (black line) to get a clearer picture of the learning trajectory. That plot also includes the rolling average of total yellow bananas collected (yellow) and blue bananas collected (blue) in order to get an idea of the raw numbers (averaged over 100 episodes) of each type of banana collected. This was to get an idea of how much further improvement was possible.\n",
    "\n",
    "### Default - Batch size: 64, Standard Model\n",
    "\n",
    "Episode 100\tAverage Score: 0.31  \n",
    "Episode 200\tAverage Score: 3.35  \n",
    "Episode 300\tAverage Score: 8.00  \n",
    "Episode 400\tAverage Score: 10.55  \n",
    "Solved in 484 episodes  \n",
    "\n",
    "![b64standard.png](Plots/b64standard.png)\n",
    "![b64standard-average.png](Plots/b64standard-average.png)\n",
    "\n",
    "### Batch Size: 64, Deep Model\n",
    "\n",
    "Episode 100\tAverage Score: 0.73  \n",
    "Episode 200\tAverage Score: 3.26  \n",
    "Episode 300\tAverage Score: 7.01  \n",
    "Episode 400\tAverage Score: 10.32\n",
    "Solved in 482 episodess  \n",
    "\n",
    "![b64deeper.png](Plots/b64deeper.png)\n",
    "![b64deeper-average.png](Plots/b64deeper-average.png)\n",
    "\n",
    "### Batch size: 256, Standard Model\n",
    "\n",
    "Episode 100\tAverage Score: 0.58  \n",
    "Episode 200\tAverage Score: 4.28  \n",
    "Episode 300\tAverage Score: 6.50  \n",
    "Episode 400\tAverage Score: 10.46  \n",
    "Solved in 475 episodes  \n",
    "\n",
    "![b256standard.png](Plots/b256standard.png)\n",
    "![b256standard-average.png](Plots/b256standard-average.png)\n",
    "\n",
    "### Batch size: 256, Deep Model\n",
    "\n",
    "Episode 100\tAverage Score: 0.88  \n",
    "Episode 200\tAverage Score: 3.27  \n",
    "Episode 300\tAverage Score: 7.24  \n",
    "Episode 400\tAverage Score: 9.66  \n",
    "Solved in 489 episodes  \n",
    "\n",
    "![b256deeper.png](Plots/b256deeper.png)\n",
    "![b256deeper-average.png](Plots/b256deeper-average.png)\n",
    "\n",
    "## Possible Improvements\n",
    "\n",
    "My first idea for improvement, which I actually tried (and failed at), was to use a deeper network described above. That was mostly in an attempt to get to the +13 solution faster. In fact, that model performed more poorly on the whole (see above).\n",
    "\n",
    "Consider this though: using the default model and parameters from the Lunar Lander assignment, the agent already learned to solve the environment in less than 500 episodes which is far below the suggested number of 1800. Given that, I did not feel a strong incentive to get the agent to solve the environment faster than that. However, I was interested to see if there were improvements which might increase the upper limit of average reward per 100 episodes.\n",
    "\n",
    "I tried to see if any of the model/parameters might perform better using a higher bar for the solution. Specifically, it seemed that reward of approximately +17 per 100 episode average was around the limit of what the particular model/parameters could acheive. \n",
    "\n",
    "### Batch size: 256, Standard Model\n",
    "\n",
    "In an alternate implementation of `navigation.ipyb` and `navigation.py` (not submitted), I track how many episodes to get to +13 (the initial assignment), +16, +17, +18, +19 and +20.\n",
    "\n",
    "Episode 100\tAverage Score: 0.58  \n",
    "Episode 200\tAverage Score: 4.28  \n",
    "Episode 300\tAverage Score: 6.50  \n",
    "Episode 400\tAverage Score: 10.46  \n",
    "Solved for +13 in 475 episodes  \n",
    "Episode 500\tAverage Score: 13.42  \n",
    "Episode 600\tAverage Score: 14.67  \n",
    "Episode 700\tAverage Score: 15.02  \n",
    "Episode 800\tAverage Score: 14.96  \n",
    "Solved for +16 in 859 episodes \n",
    "Episode 900\tAverage Score: 16.15  \n",
    "Episode 1000\tAverage Score: 15.26  \n",
    "Episode 1100\tAverage Score: 15.86  \n",
    "Episode 1200\tAverage Score: 15.74  \n",
    "Episode 1300\tAverage Score: 16.49  \n",
    "Solved for +17 in 1335 episodes  \n",
    "Episode 1400\tAverage Score: 16.44  \n",
    "Episode 1500\tAverage Score: 16.40  \n",
    "Episode 1600\tAverage Score: 15.66  \n",
    "Episode 1700\tAverage Score: 16.41  \n",
    "Episode 1800\tAverage Score: 15.62  \n",
    "Episode 1900\tAverage Score: 14.98  \n",
    "Episode 2000\tAverage Score: 15.21  \n",
    "Episode 2100\tAverage Score: 15.72  \n",
    "Episode 2200\tAverage Score: 15.22  \n",
    "Episode 2300\tAverage Score: 15.13  \n",
    "Episode 2400\tAverage Score: 14.74  \n",
    "Episode 2500\tAverage Score: 15.32  \n",
    "Episode 2600\tAverage Score: 15.00  \n",
    "Episode 2700\tAverage Score: 15.68  \n",
    "Episode 2800\tAverage Score: 16.13  \n",
    "Episode 2900\tAverage Score: 15.38  \n",
    "Episode 3000\tAverage Score: 15.62  \n",
    "Episode 3100\tAverage Score: 15.50  \n",
    "Episode 3200\tAverage Score: 14.91  \n",
    "Episode 3300\tAverage Score: 16.10  \n",
    "Episode 3400\tAverage Score: 16.41  \n",
    "Episode 3500\tAverage Score: 15.94  \n",
    "Episode 3600\tAverage Score: 15.09  \n",
    "Episode 3700\tAverage Score: 16.29  \n",
    "Episode 3800\tAverage Score: 16.15  \n",
    "Episode 3900\tAverage Score: 15.93  \n",
    "Episode 4000\tAverage Score: 15.85  \n",
    "Episode 4100\tAverage Score: 14.48  \n",
    "Episode 4200\tAverage Score: 15.53  \n",
    "Episode 4300\tAverage Score: 14.72  \n",
    "Episode 4400\tAverage Score: 15.25  \n",
    "Episode 4500\tAverage Score: 15.42  \n",
    "Episode 4600\tAverage Score: 15.07  \n",
    "Episode 4700\tAverage Score: 14.75  \n",
    "Episode 4800\tAverage Score: 15.53  \n",
    "Episode 4900\tAverage Score: 14.83  \n",
    "Episode 5000\tAverage Score: 14.77  \n",
    "\n",
    "![b256standard-17-18-19-20.png](Plots/b256standard-17-18-19-20.png)\n",
    "![b256standard-average-17-18-19-20.png](Plots/b256standard-average-17-18-19-20.png)\n",
    "\n",
    "With a batch size of 256, the standard model was able to get a +16 average on the 859th episode and +17 at the 1335th episode. The best performing model to +13 is never able to get to +18. No change in parameters was able to break +18 including:\n",
    "\n",
    "* Tau\n",
    "* Learning Rate\n",
    "* Gamma\n",
    "\n",
    "Admittedly, I had little time to devote to experimentation. Furthermore, without more detail concerning exactly what the comprises the 37 state values, it's hard to know what parameters would best be adjusted. It's mostly just guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
