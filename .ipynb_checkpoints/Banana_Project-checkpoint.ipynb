{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from unityagents import UnityEnvironment\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent # Import the Agent class\n",
    "\n",
    "agent = Agent(state_size=37, action_size=4, seed=0) # Instantiate an agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "if sys.platform == \"darwin\":\n",
    "    env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "    # If running on Mac, use Banana.app\n",
    "elif sys.platform == \"linux\":\n",
    "    env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86\")\n",
    "    # If running on Linux, use Banana.x86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# Set up the brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.92\n",
      "Episode 200\tAverage Score: 4.39\n",
      "Episode 300\tAverage Score: 7.82\n",
      "Episode 400\tAverage Score: 10.72\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cd0e643fa018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblues_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myellows_means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblues_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myellows_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save the trained model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cd0e643fa018>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m                                        \u001b[0;31m# continue the episode until done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# step the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# send the action to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# get the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes = 200000, eps_start = 1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                         # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)   # last 100 scores\n",
    "    scores_means = []                   # list containing average scores from the previous 100 scores from the last episode\n",
    "    eps = eps_start                     # initialize epsilon\n",
    "\n",
    "    blues_window = deque(maxlen=100)\n",
    "    blues_means = []\n",
    "    yellows_window = deque(maxlen=100)\n",
    "    yellows_means = []\n",
    "    \"\"\"Track yellow and blue bananas per episode and averages of each over a 100 episode window\n",
    "    \n",
    "        blues_window: number of blue bananas collected in each of the last 100 episodes\n",
    "        yellows_window: number of yellow bananas collected in each of the last 100 episodes\n",
    "        blues_means: Average number of blue bananas collected in the last 100 episodes\n",
    "        yellows_means: Average number of yellows bananas collected in the last 100 episodes\n",
    "    \"\"\"\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "        blues = 0                                          # initialize number of blue bananas\n",
    "        yellows = 0                                        # initialize number of yellow bananas\n",
    "        while True:                                        # continue the episode until done\n",
    "            action = agent.act(state, eps)                 # step the environment\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            if reward == 1:\n",
    "                yellows += 1                               # Increment yellow banana count on positive reward\n",
    "            elif reward == -1:\n",
    "                blues += 1                                 # Increment blue banana count on positive reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done) # step the agent\n",
    "            state = next_state                             # next_state becomes current state for next iteration\n",
    "            score += reward                                # update the score with the reward\n",
    "            if done:\n",
    "                break                                      # if episode is complete, break\n",
    "        blues_window.append(blues)           # add number of blues to blues window\n",
    "        yellows_window.append(yellows)       # add number of yellows to yellow window\n",
    "        scores_window.append(score)          # add most recent score to last 100\n",
    "        scores.append(score)                 # add most recent score to list of all scores\n",
    "        scores_means.append(np.mean(scores_window)) # add average of scores from the past game to the window\n",
    "        blues_means.append(np.mean(blues_window))   # get the blues average from the last 100 episodes and append to blues averages\n",
    "        yellows_means.append(np.mean(yellows_window)) # get the yellows average from the last 100 episodes and append to yellow averages\n",
    "        eps = max(eps_end, eps_decay * eps)  # decrease epsilon\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) > 13:\n",
    "            print(\"Solved in {} episodes\".format(i_episode))\n",
    "            break\n",
    "    return scores, scores_means, blues_means, yellows_means\n",
    "\n",
    "scores, scores_means, blues_means, yellows_means = dqn()\n",
    "\n",
    "agent.save() # save the trained model parameters\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "# plot the average scores and average blues and yellows for the last 100\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(len(scores_means)), scores_means, 'k-')\n",
    "ax.plot(np.arange(len(blues_means)), blues_means, 'b-')\n",
    "ax.plot(np.arange(len(yellows_means)), yellows_means, 'y-')\n",
    "plt.ylabel('Average score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial commit\n",
    "Just a basic port of the DQN solution for the new environment plus some flourishes including platform check  \n",
    "Default Performance:  \n",
    "BATCH_SIZE = 64, Deep Model  \n",
    "Episode 100\tAverage Score: 0.45  \n",
    "Episode 200\tAverage Score: 2.93  \n",
    "Episode 300\tAverage Score: 6.74  \n",
    "Episode 400\tAverage Score: 9.01  \n",
    "Episode 500\tAverage Score: 12.92  \n",
    "Solved in 502 episodes  \n",
    "\n",
    "![b64deeper.png](Plots/b64deeper.png)\n",
    "![b64deeper-average.png](Plots/b64deeper-average.png)\n",
    "\n",
    "Tuning\n",
    "\n",
    "BATCH_SIZE = 64, Standard Model  \n",
    "Episode 100\tAverage Score: 0.31  \n",
    "Episode 200\tAverage Score: 3.35  \n",
    "Episode 300\tAverage Score: 8.00  \n",
    "Episode 400\tAverage Score: 10.55  \n",
    "Solved in 484 episodes  \n",
    "\n",
    "![b64standard.png](Plots/b64standard.png)\n",
    "![b64standard-average.png](Plots/b64standard-average.png)\n",
    "\n",
    "BATCH_SIZE = 256, Standard Model  \n",
    "Episode 100\tAverage Score: 0.58  \n",
    "Episode 200\tAverage Score: 4.28  \n",
    "Episode 300\tAverage Score: 6.50  \n",
    "Episode 400\tAverage Score: 10.46  \n",
    "Solved in 475 episodes  \n",
    "\n",
    "![b256standard.png](Plots/b256standard.png)\n",
    "![b256standard-average.png](Plots/b256standard-average.png)\n",
    "\n",
    "BATCH_SIZE = 256, Standard Model, 16 Banana solution  \n",
    "Episode 100\tAverage Score: 0.58  \n",
    "Episode 200\tAverage Score: 4.28  \n",
    "Episode 300\tAverage Score: 6.50  \n",
    "Episode 400\tAverage Score: 10.46  \n",
    "Episode 500\tAverage Score: 13.42  \n",
    "Episode 600\tAverage Score: 14.67  \n",
    "Episode 700\tAverage Score: 15.02  \n",
    "Episode 800\tAverage Score: 14.96  \n",
    "Solved in 859 episodes  \n",
    "\n",
    "![b256standard-16.png](Plots/b256standard-16.png)\n",
    "![b256standard-average-16.png](Plots/b256standard-average-16.png)\n",
    "\n",
    "BATCH_SIZE = 256, Deep Model  \n",
    "Episode 100\tAverage Score: 0.91  \n",
    "Episode 200\tAverage Score: 3.69  \n",
    "Episode 300\tAverage Score: 7.45  \n",
    "Episode 400\tAverage Score: 10.17  \n",
    "Solved in 499 episodes  \n",
    "\n",
    "![b256deeper.png](Plots/b256deeper.png)\n",
    "![b256deeper-average.png](Plots/b256deeper-average.png)\n",
    "\n",
    "BATCH_SIZE = 256, Deep Model, 16 Banana Solution  \n",
    "Episode 100\tAverage Score: 0.88  \n",
    "Episode 200\tAverage Score: 3.27  \n",
    "Episode 300\tAverage Score: 7.24  \n",
    "Episode 400\tAverage Score: 9.66  \n",
    "Episode 500\tAverage Score: 13.23  \n",
    "Episode 600\tAverage Score: 14.31  \n",
    "Episode 700\tAverage Score: 14.44  \n",
    "Episode 800\tAverage Score: 14.31  \n",
    "Episode 900\tAverage Score: 15.00  \n",
    "Episode 1000\tAverage Score: 15.55  \n",
    "Episode 1100\tAverage Score: 15.76  \n",
    "Episode 1200\tAverage Score: 14.69  \n",
    "Episode 1300\tAverage Score: 14.28  \n",
    "Episode 1400\tAverage Score: 14.41  \n",
    "Episode 1500\tAverage Score: 15.42  \n",
    "Episode 1600\tAverage Score: 15.11  \n",
    "Episode 1700\tAverage Score: 15.68  \n",
    "Episode 1800\tAverage Score: 14.11  \n",
    "Episode 1900\tAverage Score: 14.43  \n",
    "Episode 2000\tAverage Score: 14.12  \n",
    "Episode 2100\tAverage Score: 15.39  \n",
    "Episode 2200\tAverage Score: 15.17  \n",
    "Episode 2300\tAverage Score: 14.74  \n",
    "Episode 2400\tAverage Score: 14.72  \n",
    "Episode 2500\tAverage Score: 15.01  \n",
    "Episode 2600\tAverage Score: 14.93  \n",
    "Episode 2700\tAverage Score: 14.82  \n",
    "Episode 2800\tAverage Score: 14.93  \n",
    "Episode 2900\tAverage Score: 14.44  \n",
    "Episode 3000\tAverage Score: 14.83  \n",
    "Episode 3100\tAverage Score: 14.13  \n",
    "Episode 3200\tAverage Score: 13.84  \n",
    "Episode 3300\tAverage Score: 14.53  \n",
    "Episode 3400\tAverage Score: 14.04  \n",
    "Episode 3500\tAverage Score: 13.56  \n",
    "![b256deeper-16.png](Plots/b256deeper-16.png)\n",
    "![b256deeper-average-16.png](Plots/b256deeper-average-16.png)\n",
    "\n",
    "BATCH_SIZE = 256, Standard Model, 20 Banana Solution  \n",
    "\n",
    "Episode 100\tAverage Score: 0.58  \n",
    "Episode 200\tAverage Score: 4.28  \n",
    "Episode 300\tAverage Score: 6.50  \n",
    "Episode 400\tAverage Score: 10.46  \n",
    "Episode 500\tAverage Score: 13.42  \n",
    "Episode 600\tAverage Score: 14.67  \n",
    "Episode 700\tAverage Score: 15.02  \n",
    "Episode 800\tAverage Score: 14.96  \n",
    "Episode 900\tAverage Score: 16.15  \n",
    "Episode 1000\tAverage Score: 15.26  \n",
    "Episode 1100\tAverage Score: 15.86  \n",
    "Episode 1200\tAverage Score: 15.74  \n",
    "Episode 1300\tAverage Score: 16.49  \n",
    "Episode 1400\tAverage Score: 16.44  \n",
    "Episode 1500\tAverage Score: 16.40  \n",
    "Episode 1600\tAverage Score: 15.66  \n",
    "Episode 1700\tAverage Score: 16.41  \n",
    "Episode 1800\tAverage Score: 15.62  \n",
    "Episode 1900\tAverage Score: 14.98  \n",
    "Episode 2000\tAverage Score: 15.21  \n",
    "Episode 2100\tAverage Score: 15.72  \n",
    "Episode 2200\tAverage Score: 15.22  \n",
    "Episode 2300\tAverage Score: 15.13  \n",
    "Episode 2400\tAverage Score: 14.74  \n",
    "Episode 2500\tAverage Score: 15.32  \n",
    "Episode 2600\tAverage Score: 15.00  \n",
    "Episode 2700\tAverage Score: 15.68  \n",
    "Episode 2800\tAverage Score: 16.13  \n",
    "Episode 2900\tAverage Score: 15.38  \n",
    "Episode 3000\tAverage Score: 15.62  \n",
    "Episode 3100\tAverage Score: 15.50  \n",
    "Episode 3200\tAverage Score: 14.91  \n",
    "Episode 3300\tAverage Score: 16.10  \n",
    "Episode 3400\tAverage Score: 16.41  \n",
    "Episode 3500\tAverage Score: 15.94  \n",
    "![b256deeper-20.png](Plots/b256deeper-20png)\n",
    "![b256deeper-average-20png](Plots/b256deeper-average-20png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
